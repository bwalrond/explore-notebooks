{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## San Francisco Crime Modeling\n",
    "\n",
    "Go here for the [details](https://www.kaggle.com/c/sf-crime) on the Kaggle competition\n",
    "\n",
    "### Predictive Goal:  \"Given time and location, you must predict the category of crime that occurred.\"\n",
    "\n",
    "Data profiling contained in a separate notebook (\"SanFranCrime.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x10bfc9490>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset from the prepared Parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "878049\n",
      "root\n",
      " |-- Dates: timestamp (nullable = true)\n",
      " |-- Category: string (nullable = true)\n",
      " |-- Descript: string (nullable = true)\n",
      " |-- DayOfWeek: string (nullable = true)\n",
      " |-- PdDistrict: string (nullable = true)\n",
      " |-- Resolution: string (nullable = true)\n",
      " |-- Address: string (nullable = true)\n",
      " |-- X: float (nullable = true)\n",
      " |-- Y: float (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "parqFileName = '/Users/bill.walrond/Documents/dsprj/data/SanFranCrime/train.pqt'\n",
    "sfc_train = sqlContext.read.parquet(parqFileName)\n",
    "print sfc_train.count()\n",
    "print sfc_train.printSchema()\n",
    "sfc_train = sfc_train.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1:  Establish and evaluate a baseline \n",
    "\n",
    "From the profiling results, the most frequent category of crime by far is \"LARCENY/THEFT\".  We can set our baseline prediction to assume every crime is LARCENY/THEFT regardless of the actual category or any of the other attributes.  Then, evaluate how accurate our baseline preditions are.  Later, we will compare how much better/worse the machine learning methods are compared to this baseline.\n",
    "\n",
    "For now, we're going to start with Precision-Recall for our evaluation framework.  Later, we may consider additional evaluation metrics (e.g. AUC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.199192\n",
      "Recall:  0.199192\n"
     ]
    }
   ],
   "source": [
    "# Index labels, adding metadata to the label column.\n",
    "# Fit on whole dataset to include all labels in index.\n",
    "labelIndexer = StringIndexer(inputCol=\"Category\", outputCol=\"indexedLabel\").fit(sfc_train)\n",
    "sfc_train_t = labelIndexer.transform(sfc_train)\n",
    "sfc_train_t = sfc_train_t.cache()\n",
    "\n",
    "baseline_preds = sfc_train_t.selectExpr('indexedLabel as prediction', 'double(0) as label')\n",
    "baseline_preds = baseline_preds.cache()\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol='prediction') \n",
    "evaluator.evaluate(baseline_preds) \n",
    "print 'Precision: {:08.6f}'.format(evaluator.evaluate(baseline_preds, {evaluator.metricName: 'precision'}))\n",
    "print 'Recall:  {:08.6f}'.format(evaluator.evaluate(baseline_preds, {evaluator.metricName: 'recall'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thus, our machine learning results must be better than guessing that every category is LARCENY/THEFT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2:  Prepare the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sampdatafile = '/Users/bill.walrond/opt/spark-1.6.1-bin-hadoop2.6/data/mllib/sample_libsvm_data.txt'\n",
    "sampdata = sqlContext.read.format(\"libsvm\").load(sampdatafile)\n",
    "sampdata.head(1)\n",
    "# the sampdata is read and transformed directly into rows of label and features(SparseVector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|label|            features|             indexed|\n",
      "+-----+--------------------+--------------------+\n",
      "|  0.0|(692,[127,128,129...|(692,[127,128,129...|\n",
      "|  1.0|(692,[158,159,160...|(692,[158,159,160...|\n",
      "|  1.0|(692,[124,125,126...|(692,[124,125,126...|\n",
      "|  1.0|(692,[152,153,154...|(692,[152,153,154...|\n",
      "|  1.0|(692,[151,152,153...|(692,[151,152,153...|\n",
      "|  0.0|(692,[129,130,131...|(692,[129,130,131...|\n",
      "|  1.0|(692,[158,159,160...|(692,[158,159,160...|\n",
      "|  1.0|(692,[99,100,101,...|(692,[99,100,101,...|\n",
      "|  0.0|(692,[154,155,156...|(692,[154,155,156...|\n",
      "|  0.0|(692,[127,128,129...|(692,[127,128,129...|\n",
      "|  1.0|(692,[154,155,156...|(692,[154,155,156...|\n",
      "|  0.0|(692,[153,154,155...|(692,[153,154,155...|\n",
      "|  0.0|(692,[151,152,153...|(692,[151,152,153...|\n",
      "|  1.0|(692,[129,130,131...|(692,[129,130,131...|\n",
      "|  0.0|(692,[154,155,156...|(692,[154,155,156...|\n",
      "|  1.0|(692,[150,151,152...|(692,[150,151,152...|\n",
      "|  0.0|(692,[124,125,126...|(692,[124,125,126...|\n",
      "|  0.0|(692,[152,153,154...|(692,[152,153,154...|\n",
      "|  1.0|(692,[97,98,99,12...|(692,[97,98,99,12...|\n",
      "|  1.0|(692,[124,125,126...|(692,[124,125,126...|\n",
      "+-----+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexed\", maxCategories=10)\n",
    "indexerModel = indexer.fit(sampdata)\n",
    "\n",
    "# Create new column \"indexed\" with categorical values transformed to indices\n",
    "indexedData = indexerModel.transform(sampdata)\n",
    "indexedData.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------------+\n",
      "| id|category|categoryIndex|\n",
      "+---+--------+-------------+\n",
      "|  0|       a|          0.0|\n",
      "|  1|       b|          2.0|\n",
      "|  2|       c|          1.0|\n",
      "|  3|       a|          0.0|\n",
      "|  4|       a|          0.0|\n",
      "+---+--------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n",
      "+---+-------------+\n",
      "|id |categoryVec  |\n",
      "+---+-------------+\n",
      "|0  |(3,[0],[1.0])|\n",
      "|1  |(3,[2],[1.0])|\n",
      "|2  |(3,[1],[1.0])|\n",
      "|3  |(3,[0],[1.0])|\n",
      "|4  |(3,[0],[1.0])|\n",
      "|5  |(3,[1],[1.0])|\n",
      "+---+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "df = sqlContext.createDataFrame([\n",
    "    (0, \"a\"),\n",
    "    (1, \"b\"),\n",
    "    (2, \"c\"),\n",
    "    (3, \"a\"),\n",
    "    (4, \"a\"),\n",
    "    (5, \"c\")\n",
    "], [\"id\", \"category\"])\n",
    "\n",
    "stringIndexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\")\n",
    "model = stringIndexer.fit(df)\n",
    "indexed = model.transform(df)\n",
    "print indexed.show(5)\n",
    "encoder = OneHotEncoder(dropLast=False, inputCol=\"categoryIndex\", outputCol=\"categoryVec\")\n",
    "encoded = encoder.transform(indexed)\n",
    "encoded.select(\"id\", \"categoryVec\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Dates: timestamp (nullable = true)\n",
      " |-- Category: string (nullable = true)\n",
      " |-- Descript: string (nullable = true)\n",
      " |-- DayOfWeek: string (nullable = true)\n",
      " |-- PdDistrict: string (nullable = true)\n",
      " |-- Resolution: string (nullable = true)\n",
      " |-- Address: string (nullable = true)\n",
      " |-- X: float (nullable = true)\n",
      " |-- Y: float (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print sfc_train.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the categorical features ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "878049\n",
      "root\n",
      " |-- Dates: timestamp (nullable = true)\n",
      " |-- Category: string (nullable = true)\n",
      " |-- Descript: string (nullable = true)\n",
      " |-- DayOfWeek: string (nullable = true)\n",
      " |-- PdDistrict: string (nullable = true)\n",
      " |-- Resolution: string (nullable = true)\n",
      " |-- Address: string (nullable = true)\n",
      " |-- X: float (nullable = true)\n",
      " |-- Y: float (nullable = true)\n",
      " |-- DescriptIndex: double (nullable = true)\n",
      " |-- DescriptVec: vector (nullable = true)\n",
      " |-- DayOfWeekIndex: double (nullable = true)\n",
      " |-- DayOfWeekVec: vector (nullable = true)\n",
      " |-- PdDistrictIndex: double (nullable = true)\n",
      " |-- PdDistrictVec: vector (nullable = true)\n",
      " |-- ResolutionIndex: double (nullable = true)\n",
      " |-- ResolutionVec: vector (nullable = true)\n",
      " |-- AddressIndex: double (nullable = true)\n",
      " |-- AddressVec: vector (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "cols = ['Descript','DayOfWeek','PdDistrict','Resolution','Address']\n",
    "\n",
    "for col in cols:\n",
    "    stringIndexer = StringIndexer(inputCol=col, outputCol=col+'Index')\n",
    "    model = stringIndexer.fit(sfc_train)\n",
    "    sfc_train = model.transform(sfc_train)\n",
    "    encoder = OneHotEncoder(dropLast=False, inputCol=col+'Index', outputCol=col+'Vec')\n",
    "    sfc_train = encoder.transform(sfc_train)\n",
    "\n",
    "print sfc_train.count()\n",
    "print sfc_train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+------------+--------------------+\n",
      "|Address                      |AddressIndex|AddressVec          |\n",
      "+-----------------------------+------------+--------------------+\n",
      "|VANNESS AV / GREENWICH ST    |7781.0      |(23228,[7781],[1.0])|\n",
      "|JEFFERSON ST / LEAVENWORTH ST|6049.0      |(23228,[6049],[1.0])|\n",
      "|MENDELL ST / HUDSON AV       |5846.0      |(23228,[5846],[1.0])|\n",
      "|2000 Block of BUSH ST        |3197.0      |(23228,[3197],[1.0])|\n",
      "|1600 Block of WEBSTER ST     |3081.0      |(23228,[3081],[1.0])|\n",
      "|0 Block of STOCKTON ST       |72.0        |(23228,[72],[1.0])  |\n",
      "|23RD ST / WISCONSIN ST       |4847.0      |(23228,[4847],[1.0])|\n",
      "|GEARY BL / LAGUNA ST         |246.0       |(23228,[246],[1.0]) |\n",
      "|400 Block of HYDE ST         |417.0       |(23228,[417],[1.0]) |\n",
      "|STOCKTON ST / SUTTER ST      |441.0       |(23228,[441],[1.0]) |\n",
      "+-----------------------------+------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sfc_train.select('Address','AddressIndex','AddressVec').show(10,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assembling the feature vector ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------+\n",
      "|features                                                                                              |\n",
      "+------------------------------------------------------------------------------------------------------+\n",
      "|(24143,[44,880,888,897,8694,24141,24142],[1.0,1.0,1.0,1.0,1.0,-122.42436218261719,37.8004150390625])  |\n",
      "|(24143,[8,880,890,896,6962,24141,24142],[1.0,1.0,1.0,1.0,1.0,-122.4190902709961,37.80780029296875])   |\n",
      "|(24143,[10,880,889,897,6759,24141,24142],[1.0,1.0,1.0,1.0,1.0,-122.38639831542969,37.738983154296875])|\n",
      "|(24143,[0,880,888,896,4110,24141,24142],[1.0,1.0,1.0,1.0,1.0,-122.43101501464844,37.78738784790039])  |\n",
      "|(24143,[12,880,888,896,3994,24141,24142],[1.0,1.0,1.0,1.0,1.0,-122.43131256103516,37.78586959838867]) |\n",
      "|(24143,[19,880,891,896,985,24141,24142],[1.0,1.0,1.0,1.0,1.0,-122.4063491821289,37.78602981567383])   |\n",
      "|(24143,[121,880,889,896,5760,24141,24142],[1.0,1.0,1.0,1.0,1.0,-122.39869689941406,37.75474548339844])|\n",
      "|(24143,[0,880,888,896,1159,24141,24142],[1.0,1.0,1.0,1.0,1.0,-122.42799377441406,37.78495407104492])  |\n",
      "|(24143,[85,880,891,896,1330,24141,24142],[1.0,1.0,1.0,1.0,1.0,-122.4162368774414,37.784912109375])    |\n",
      "|(24143,[0,880,890,896,1354,24141,24142],[1.0,1.0,1.0,1.0,1.0,-122.4069595336914,37.78943634033203])   |\n",
      "+------------------------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "vector_cols = [name for name,type in sfc_train.dtypes if 'Vec' in name ] + ['X','Y']\n",
    "assembler = VectorAssembler(inputCols=vector_cols, outputCol=\"features\")\n",
    "output = assembler.transform(sfc_train)\n",
    "print output.select('features').show(10,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
