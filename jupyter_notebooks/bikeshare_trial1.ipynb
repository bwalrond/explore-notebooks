{"cells":[{"cell_type":"markdown","source":["### Import some useful libraries ..."],"metadata":{}},{"cell_type":"code","source":["import sys\nfrom pyspark.sql import Row, Window\nimport pyspark.sql.functions as SQLFunctions"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":["## This won't work ..."],"metadata":{}},{"cell_type":"code","source":["bikedatafile = \"bikeshare_train\"\nbike_data = sc.textFile(bikedatafile).cache()\nline_count = bike_data.count()\nprint \"Line count is %d\" % line_count"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["## ... but this does\n(I'm still not sure the best way to get dataframe from an file that I uploaded but this will suffice)"],"metadata":{}},{"cell_type":"code","source":["# df = sqlContext.sql(\"select * from bikeshare_test\")\ndf = sqlContext.sql(\"select * from bikeshare_train union select * from bikeshare_test\")\nprint df.count()\nprint df.printSchema()\nprint df.describe().show(n=10)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["## And now, some Feature extraction ...\n\nSince the fit() functions all expect two input columns, a label column and a feature column that combines all the encoded features together, we have some steps to take first.\nUse the VectorAssembler to take a group of separate integer columns and create a new column that is a vector combination of the integer columns"],"metadata":{}},{"cell_type":"code","source":["from pyspark.mllib.linalg import Vectors\nfrom pyspark.ml.feature import VectorAssembler\n\nassembler = VectorAssembler(\n    inputCols=[\"temp\", \"atemp\", \"humidity\",\"windspeed\"],\n    outputCol=\"features\")\noutput = assembler.transform(df)\n# transform the count column from integer to double (since the labels must be a double)\noutput = output.select((output[\"count\"]*1.0).alias(\"label\"), \"*\")\n# Let's print out a few of the rows to see how if the VectorAssembler did its job\nprint output.select(\"count\",\"temp\", \"atemp\", \"humidity\",\"windspeed\",\"features\").show(n=5,truncate=False)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["### Now we have a column for 'labels' and a column for 'features'\n... which means we have what we need to fit/learn a model."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.regression import LinearRegression\nlr = LinearRegression(maxIter=5)\nmodel = lr.fit(output)\nprint model"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["from pyspark.ml import Pipeline\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.feature import HashingTF, Tokenizer\nfrom pyspark.sql import Row\n\n# Prepare training documents from a list of (id, text, label) tuples.\ntraining = sqlContext.createDataFrame([\n    (0L, \"a b c d e spark\", 1.0),\n    (1L, \"b d\", 0.0),\n    (2L, \"spark f g h\", 1.0),\n    (3L, \"hadoop mapreduce\", 0.0)], [\"id\", \"text\", \"label\"])\n\n# Configure an ML pipeline, which consists of tree stages: tokenizer, hashingTF, and lr.\ntokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\nhashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\nlr = LogisticRegression(maxIter=10, regParam=0.01)\npipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n\n# Fit the pipeline to training documents.\nmodel = pipeline.fit(training)\n\n# Prepare test documents, which are unlabeled (id, text) tuples.\ntest = sqlContext.createDataFrame([\n    (4L, \"spark i j k\"),\n    (5L, \"l m n\"),\n    (6L, \"mapreduce spark\"),\n    (7L, \"apache hadoop\")], [\"id\", \"text\"])\n\n# Make predictions on test documents and print columns of interest.\nprediction = model.transform(test)\nselected = prediction.select(\"id\", \"text\", \"prediction\")\nfor row in selected.collect():\n    print(row)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["from pyspark.ml import Pipeline\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.feature import HashingTF, Tokenizer\nfrom pyspark.sql import Row\n\n# Prepare training documents from a list of (id, text, label) tuples.\ntraining = sqlContext.createDataFrame([\n    (0L, \"a b c d e spark\", 1.0),\n    (1L, \"b d\", 0.0),\n    (2L, \"spark f g h\", 1.0),\n    (3L, \"hadoop mapreduce\", 0.0)], [\"id\", \"text\", \"label\"])\n\nprint \"Pre-transformed training set ...\"\nprint training.show()\n# Configure an ML pipeline, which consists of tree stages: tokenizer, hashingTF, and lr.\ntokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\ntr = tokenizer.transform(training)\nprint \"Transformed training set ...\"\nprint tr.show()\nhashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\nhashed = hashingTF.transform(tr)\nprint \"Hashed data set ...\"\nprint hashed.show()\n# lr = LogisticRegression(maxIter=10, regParam=0.01)\n# pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["from numpy import allclose\nfrom pyspark.mllib.linalg import Vectors\nfrom pyspark.ml.feature import StringIndexer\ndf = sqlContext.createDataFrame([\n    (1.0, Vectors.dense(1.0)),\n    (0.0, Vectors.sparse(1, [], []))], [\"label\", \"features\"])\nstringIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexed\")\nsi_model = stringIndexer.fit(df)\ntd = si_model.transform(df)\nprint td.show()\n# >>> gbt = GBTClassifier(maxIter=5, maxDepth=2, labelCol=\"indexed\")\n# >>> model = gbt.fit(td)\n# >>> allclose(model.treeWeights, [1.0, 0.1, 0.1, 0.1, 0.1])\n# True\n# >>> test0 = sqlContext.createDataFrame([(Vectors.dense(-1.0),)], [\"features\"])\n# >>> model.transform(test0).head().prediction\n# 0.0\n# >>> test1 = sqlContext.createDataFrame([(Vectors.sparse(1, [0], [1.0]),)], [\"features\"])\n# >>> model.transform(test1).head().prediction\n# 1.0"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":14}],"metadata":{"name":"bikeshare_trial1","notebookId":16726},"nbformat":4,"nbformat_minor":0}
